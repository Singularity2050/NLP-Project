{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "private_outputs": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers pandas\n",
        "!pip install matplotlib"
      ],
      "metadata": {
        "id": "uEip1OvWRytp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSY5b5ghOZi7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from transformers import BertTokenizerFast,AutoModel,BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "import requests\n",
        "import zipfile\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import itertools\n",
        "from torch.nn import CrossEntropyLoss\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "request = requests.get(\"https://github.com/MHDBST/PerSenT/archive/refs/heads/main.zip\")\n",
        "with open(\"data.zip\", \"wb\") as file:\n",
        "    file.write(request.content)\n",
        "\n",
        "# Unzip data\n",
        "with zipfile.ZipFile('data.zip') as zip:\n",
        "    zip.extractall('data')"
      ],
      "metadata": {
        "id": "AiXx5fvuPE1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "E3Eynn1FRrkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This class is responsible for reading the CSV file containing paragraph data,\n",
        "# tokenizing paragraphs using the provided tokenizer, and returning the tokenized input along with the sentiment label.\n",
        "class PerSentParagraphDataset(Dataset):\n",
        "    def __init__(self, path, tokenizer, max_length=150): #256\n",
        "        # seperate data by columns.\n",
        "        self.data = pd.read_csv(path, delimiter=',')\n",
        "        self.data.dropna(subset=['DOCUMENT','TRUE_SENTIMENT'], inplace=False)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Collect paragraphs and their sentiments\n",
        "        self.paragraphs = []\n",
        "        self.sentiments = []\n",
        "        \n",
        "        for _, row in self.data.iterrows():\n",
        "            #devide each paragraph from document.\n",
        "            paragraphs = row['DOCUMENT'].split('\\n')\n",
        "            for i in range(len(paragraphs)):\n",
        "                if(i >=16):\n",
        "                  break\n",
        "                paragraph_col = f'Paragraph{i}'\n",
        "                if pd.notna(row[paragraph_col]):\n",
        "                    #store its paragraph\n",
        "                    self.paragraphs.append(paragraphs[i])\n",
        "                    #get each sentiments \n",
        "                    self.sentiments.append(row[paragraph_col])\n",
        "                # if there is one paragraph, then that paragraph is the document sentiment.\n",
        "                elif (len(paragraphs) == 1): \n",
        "                    self.sentiments.append(row['TRUE_SENTIMENT'])  \n",
        "                    self.paragraphs.append(paragraphs[i])\n",
        "                    \n",
        "    def __len__(self):\n",
        "        return len(self.paragraphs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        paragraph = self.paragraphs[idx]\n",
        "        true_sentiment = self.sentiments[idx]\n",
        "        # make paragraph as caculatable form\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            paragraph,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        # laveling\n",
        "        label_map = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
        "        label = torch.tensor(label_map[true_sentiment], dtype=torch.long)\n",
        "        \n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].squeeze(),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
        "            'labels': label\n",
        "        }\n"
      ],
      "metadata": {
        "id": "E7aAzx4bObEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def plot_confusion_matrix(cm, target_names):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.get_cmap('Blues'))\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(target_names))\n",
        "    plt.xticks(tick_marks, target_names, rotation=45)\n",
        "    plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kbDhBFKZwn5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and validation function for combined datasets\n",
        "def fine_tune(model, train_loader, val_loader, num_epochs=4, device=None):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "    weighted_loss = CrossEntropyLoss(weight=torch.tensor([3.0, 1.0, 1.0]).to(device))\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training loop\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # Validation loop\n",
        "        true_labels = []\n",
        "        pred_labels = []\n",
        "        model.eval()\n",
        "        total_correct = 0\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                _, predictions = torch.max(outputs.logits, dim=1) # argMax\n",
        "                total_correct += torch.sum(predictions == labels).item()\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "            pred_labels.extend(predictions.cpu().numpy())\n",
        "\n",
        "        accuracy = total_correct / len(val_loader.dataset)\n",
        "        report = classification_report(true_labels, pred_labels, target_names=['Negative', 'Neutral', 'Positive'])\n",
        "        cm = confusion_matrix(true_labels, pred_labels)\n",
        "        \n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
        "        print('Report: ');\n",
        "        print(report)\n",
        "        print('Confusion Matrix:')\n",
        "        plot_confusion_matrix(cm, target_names=['Negative', 'Neutral', 'Positive'])\n",
        "\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "NKRMBKpcR1b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Load BERT and tokenizer: The pre-trained BERT model and tokenizer are loaded\n",
        "# # using the 'BertTokenizerFast' classes from the Hugging Face Transformer library\n",
        "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "train_dataset = PerSentParagraphDataset('data/PerSenT-main/train.csv', tokenizer)\n",
        "val_dataset = PerSentParagraphDataset('data/PerSenT-main/dev.csv', tokenizer)\n",
        "# # Create data loaders: Train and validation datasets are created using the custom dataset class.\n",
        "# # Data loaders are created with a specified batch size and shuffling for the traing set.\n",
        "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=100, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "a8f9VGQqOdKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#initilize model and optimizer: The Bert model for sequence classification is initialized with three output lavels.\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
        "fine_tuned_model = fine_tune(model, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "I9scJroJOmaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "fine_tuned_model.save_pretrained('fine_tuned_bert_sentiment_elm')\n",
        "\n",
        "# model.save_pretrained('fine_tuned_bert_sentiment')"
      ],
      "metadata": {
        "id": "60RWqb0MPcu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('fine_tuned_bert_sentiment_elm')\n",
        "\n",
        "def predict_paragraph_sentiments(document):\n",
        "    paragraph_sentiments = []\n",
        "    paragraphs = document.split('\\n')\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "    for paragraph in paragraphs:\n",
        "        if len(paragraph.strip()) > 0:\n",
        "            inputs = tokenizer.encode_plus(\n",
        "                paragraph,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                max_length=130,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                _, predictions = torch.max(outputs.logits, dim=1)\n",
        "                sentiment = ['Negative', 'Neutral', 'Positive'][predictions.item()]\n",
        "                paragraph_sentiments.append(sentiment)\n",
        "\n",
        "    return paragraph_sentiments\n",
        "\n",
        "def predict_document_sentiment(document):\n",
        "    paragraph_sentiments = predict_paragraph_sentiments(document)\n",
        "    # Positive -> Negative -> Neutral (if they have same number)\n",
        "    sentiment_count = {'Positive': 0,'Negative': 0, 'Neutral': 0}\n",
        "    \n",
        "    for sentiment in paragraph_sentiments:\n",
        "        sentiment_count[sentiment] += 1\n",
        "    return max(sentiment_count, key=sentiment_count.get)\n",
        "    "
      ],
      "metadata": {
        "id": "5HS3SC7lPpHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate_document(path):\n",
        "    data = pd.read_csv(path, delimiter=',')\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "\n",
        "    for _, row in data.iterrows():\n",
        "        true_labels.append(row['TRUE_SENTIMENT'])\n",
        "        pred_labels.append(predict_document_sentiment(row['DOCUMENT']))\n",
        "\n",
        "    label_map = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
        "    true_labels = [label_map[label] for label in true_labels]\n",
        "    pred_labels = [label_map[label] for label in pred_labels]\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, pred_labels)\n",
        "    recall = recall_score(true_labels, pred_labels, average=None)\n",
        "    f1 = f1_score(true_labels, pred_labels, average=None)\n",
        "    f1_macro = f1_score(true_labels, pred_labels, average='macro')\n",
        "    recall_macro = recall_score(true_labels, pred_labels, average='macro')\n",
        "    \n",
        "    return accuracy, recall, f1, recall_macro, f1_macro\n"
      ],
      "metadata": {
        "id": "oM34z-f59-Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy, recall, f1, recall_macro, f1_macro = evaluate_document('data/PerSenT-main/fixed_test.csv')\n",
        "\n",
        "data = {\n",
        "    'Metric': ['Accuracy', 'Recall (Negative)', 'Recall (Neutral)', 'Recall (Positive)', 'Macro Recall', 'F1 (Negative)', 'F1 (Neutral)', 'F1 (Positive)', 'Macro F1'],\n",
        "    'Value': [accuracy, recall[0], recall[1], recall[2], recall_macro, f1[0], f1[1], f1[2], f1_macro]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(data)\n",
        "print(results_df)\n",
        "# print(f'Test accuracy: {accuracy:.4f} recall: {recall:.4f} f1-score: {f1:.4f}')"
      ],
      "metadata": {
        "id": "cPWEQyUA9yqV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}