{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers pandas\n",
        "!pip install matplotlib"
      ],
      "metadata": {
        "id": "uEip1OvWRytp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSY5b5ghOZi7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from transformers import BertTokenizerFast,AutoModel,BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "import requests\n",
        "import zipfile\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import itertools\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "request = requests.get(\"https://github.com/MHDBST/PerSenT/archive/refs/heads/main.zip\")\n",
        "with open(\"data.zip\", \"wb\") as file:\n",
        "    file.write(request.content)\n",
        "\n",
        "# Unzip data\n",
        "with zipfile.ZipFile('data.zip') as zip:\n",
        "    zip.extractall('data')"
      ],
      "metadata": {
        "id": "AiXx5fvuPE1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "E3Eynn1FRrkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This class is responsible for reading the CSV file containing paragraph data,\n",
        "# tokenizing paragraphs using the provided tokenizer, and returning the tokenized input along with the sentiment label.\n",
        "class PerSentParagraphDataset(Dataset):\n",
        "    def __init__(self, path, tokenizer, max_length=150): #256\n",
        "        # seperate data by columns.\n",
        "        self.data = pd.read_csv(path, delimiter=',')\n",
        "        self.data.dropna(subset=['DOCUMENT','TRUE_SENTIMENT'], inplace=False)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Collect paragraphs and their sentiments\n",
        "        self.paragraphs = []\n",
        "        self.sentiments = []\n",
        "        \n",
        "        for _, row in self.data.iterrows():\n",
        "            #devide each paragraph from document.\n",
        "            paragraphs = row['DOCUMENT'].split('\\n')\n",
        "            for i in range(len(paragraphs)):\n",
        "                if(i >=16):\n",
        "                  break\n",
        "                paragraph_col = f'Paragraph{i}'\n",
        "                if pd.notna(row[paragraph_col]):\n",
        "                    #store its paragraph\n",
        "                    self.paragraphs.append(paragraphs[i])\n",
        "                    #get each sentiments \n",
        "                    self.sentiments.append(row[paragraph_col])\n",
        "                # if there is one paragraph, then that paragraph is the document sentiment.\n",
        "                elif (len(paragraphs) == 1): \n",
        "                    self.sentiments.append(row['TRUE_SENTIMENT'])  \n",
        "                    self.paragraphs.append(paragraphs[i])\n",
        "                    \n",
        "    def __len__(self):\n",
        "        return len(self.paragraphs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        paragraph = self.paragraphs[idx]\n",
        "        true_sentiment = self.sentiments[idx]\n",
        "        # make paragraph as caculatable form\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            paragraph,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        # laveling\n",
        "        label_map = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
        "        label = torch.tensor(label_map[true_sentiment], dtype=torch.long)\n",
        "        \n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].squeeze(),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
        "            'labels': label\n",
        "        }\n"
      ],
      "metadata": {
        "id": "E7aAzx4bObEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def plot_confusion_matrix(cm, target_names):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.get_cmap('Blues'))\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(target_names))\n",
        "    plt.xticks(tick_marks, target_names, rotation=45)\n",
        "    plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kbDhBFKZwn5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and validation function for combined datasets\n",
        "def fine_tune(model, train_loader, val_loader, num_epochs=4, device=None):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training loop\n",
        "        model.train()\n",
        "        i = 0\n",
        "        for batch in train_loader:\n",
        "            print(i)\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            i = i +1\n",
        "\n",
        "        # Validation loop\n",
        "        true_labels = []\n",
        "        pred_labels = []\n",
        "        model.eval()\n",
        "        total_correct = 0\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                _, predictions = torch.max(outputs.logits, dim=1) # argMax\n",
        "                total_correct += torch.sum(predictions == labels).item()\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "            pred_labels.extend(predictions.cpu().numpy())\n",
        "\n",
        "        accuracy = total_correct / len(val_loader.dataset)\n",
        "        report = classification_report(true_labels, pred_labels, target_names=['Negative', 'Neutral', 'Positive'])\n",
        "        cm = confusion_matrix(true_labels, pred_labels)\n",
        "        \n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Report: {report}')\n",
        "        print('Confusion Matrix:')\n",
        "        plot_confusion_matrix(cm, target_names=['Negative', 'Neutral', 'Positive'])\n",
        "\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "NKRMBKpcR1b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Load BERT and tokenizer: The pre-trained BERT model and tokenizer are loaded\n",
        "# # using the 'BertTokenizerFast' classes from the Hugging Face Transformer library\n",
        "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "train_dataset = PerSentParagraphDataset('data/PerSenT-main/train.csv', tokenizer)\n",
        "val_dataset = PerSentParagraphDataset('data/PerSenT-main/dev.csv', tokenizer)\n",
        "# # Create data loaders: Train and validation datasets are created using the custom dataset class.\n",
        "# # Data loaders are created with a specified batch size and shuffling for the traing set.\n",
        "train_loader = DataLoader(train_dataset, batch_size=350, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=350, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "a8f9VGQqOdKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initilize model and optimizer: The Bert model for sequence classification is initialized with three output lavels.\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
        "fine_tuned_model = fine_tune(model, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "I9scJroJOmaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "fine_tuned_model.save_pretrained('fine_tuned_bert_sentiment_elm')\n",
        "\n",
        "# model.save_pretrained('fine_tuned_bert_sentiment')"
      ],
      "metadata": {
        "id": "60RWqb0MPcu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('fine_tuned_bert_sentiment_elm')\n",
        "\n",
        "def predict_paragraph_sentiments(document):\n",
        "    paragraph_sentiments = []\n",
        "    paragraphs = document.split('\\n')\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "    for paragraph in paragraphs:\n",
        "        if len(paragraph.strip()) > 0:\n",
        "            inputs = tokenizer.encode_plus(\n",
        "                paragraph,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                max_length=130,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                _, predictions = torch.max(outputs.logits, dim=1)\n",
        "                sentiment = ['Negative', 'Neutral', 'Positive'][predictions.item()]\n",
        "                paragraph_sentiments.append(sentiment)\n",
        "\n",
        "    return paragraph_sentiments\n",
        "\n",
        "def predict_document_sentiment(document):\n",
        "    paragraph_sentiments = predict_paragraph_sentiments(document)\n",
        "    # Positive -> Negative -> Neutral (if they have same number)\n",
        "    sentiment_count = {'Positive': 0,'Negative': 0, 'Neutral': 0}\n",
        "    \n",
        "    for sentiment in paragraph_sentiments:\n",
        "        sentiment_count[sentiment] += 1\n",
        "    print(sentiment_count)\n",
        "    return max(sentiment_count, key=sentiment_count.get)\n",
        "    \n",
        "\n",
        "sample_document = \"\"\"HOUSTON  Pa. (Reuters) - The conservative southwest corner of Pennsylvania  a patchwork of small towns  farms and Pittsburgh suburbs where voters backed Republican Donald Trump by 20 percentage points in 2016  seems an unlikely spot for a possible Democratic renaissance.\n",
        "In a district where  Trump  remains popular and Democrats have not even fielded a candidate in the previous two congressional elections  Saccone has tied himself closely to the president. The conservative 59-year-old state legislator has joked he ‚Äúwas  Trump  before  Trump  was  Trump .‚Äù\n",
        "Democrat Lamb  who hails from a prominent Pennsylvania political family  rarely mentions  Trump   focusing on economic issues  healthcare and protecting Social Security and Medicare.\n",
        "To head off Republican charges he would be a loyal follower of Democratic House of Representatives leader Nancy Pelosi  he said he would not support her for speaker and has promised to work with  Trump  if it would help the district.\"\"\"\n",
        "\n",
        "predicted_paragraph_sentiments = predict_paragraph_sentiments(sample_document)\n",
        "print(f'Predicted paragraph sentiments: {predicted_paragraph_sentiments}')\n",
        "\n",
        "predicted_document_sentiment = predict_document_sentiment(sample_document)\n",
        "print(f'Predicted document sentiment: {predicted_document_sentiment}')\n"
      ],
      "metadata": {
        "id": "5HS3SC7lPpHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  \n",
        "def evaluate_document(path):\n",
        "    data = pd.read_csv(path, delimiter=',')\n",
        "    count = 0\n",
        "    numRow = 0\n",
        "    for _, row in data.iterrows():\n",
        "        if(predict_document_sentiment(row['DOCUMENT']) == row['TRUE_SENTIMENT']):\n",
        "          count +=1\n",
        "        numRow += 1\n",
        "    return count/numRow\n"
      ],
      "metadata": {
        "id": "oM34z-f59-Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy = evaluate_document('data/PerSenT-main/fixed_test.csv')\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')"
      ],
      "metadata": {
        "id": "cPWEQyUA9yqV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}